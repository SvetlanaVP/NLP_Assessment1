{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/svetlana/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/svetlana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/svetlana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PRAW in /home/svetlana/.local/lib/python3.10/site-packages (7.7.1)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /home/svetlana/.local/lib/python3.10/site-packages (from PRAW) (1.6.3)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/svetlana/.local/lib/python3.10/site-packages (from PRAW) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/svetlana/.local/lib/python3.10/site-packages (from PRAW) (2.4.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /home/svetlana/.local/lib/python3.10/site-packages (from prawcore<3,>=2.1->PRAW) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/svetlana/.local/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/svetlana/.local/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (1.25.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/svetlana/.local/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/svetlana/.local/lib/python3.10/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "from collections import Counter\n",
    "import requests\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "from matplotlib import *\n",
    "!pip install PRAW\n",
    "import numpy as np\n",
    "import praw\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The news is all around us - on social media, newspapers, radio. Research shows that constant exposure to news can [make people feel stressed and anxious](https://www.apa.org/monitor/2022/11/strain-media-overload), but does it also affect how they respond to it? This notebook will attempt to begin to answer that question by gathering data from the subreddit r/worldnews, tokenising and lemmatising it, and analysing the comments using the VAD model to see which comments score the highest and/or lowest.\n",
    "\n",
    "NLP is useful to us in answering this question, both by providing the tools we need (e.g. the NLTK library and VAD model) but also for its ability to work with lots of data. Since our question revolves news as a whole, rather than a specific incident, we need to be able to gather enough data from multiple pieces of news to analyse people's responses.\n",
    "\n",
    "In this case, I decided to use the Reddit API to gather all top-level comments (i.e. immediate responses rather than reply chains) from the top 10 posts of the past year on the subreddit r/worldnews. The choice of subreddit was in part due to its popularity (to avoid having to scrape lots of posts), and also to try and counteract the biggest problem with using Reddit as a source - how US centric it is.  Nearly 50% of Reddit traffic [comes from the USA](https://www.statista.com/statistics/325144/reddit-global-active-user-distribution/), where the US only accounts for [about 4% of the world population](https://www.worldometers.info/world-population/us-population/). The subreddit r/worldnews, however, specifically bans posts that relate to US-internal matters, hopefully meaning that our sample is more representative of news topics worldwide and individuals' reactions to them.\n",
    "\n",
    "However, just because the sample size is large (and if run correctly, this notebook should result in a corpus of 4490 comments) doesn't mean it's not biased - these comments all still come from the same 10 posts, all of which were popular even by the subreddit's standards, and thus could be considered outliers. This could transfer to the comments VAD scores if they were reacting to particularly noteworthy news, compared to events one might read about in a local newspaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data via Reddit API Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(user_agent='VAD',\n",
    "                     client_id='5fKrQPC9VKrFf3F00g4VnQ', client_secret=\"kDxCm9NjR_D3QkfatcoR-M6cNy8j-A\",\n",
    "                     username='crochet9000', password='lkjhgfdsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code connects to the Reddit API and gathers the top-level comment data for the post url provided.\n",
    "#All code in the cell originally by James Carney (module lead) with permission to use in this assignment\n",
    "def submission(submission_id):  \n",
    "    try:\n",
    "        submission = reddit.submission(url = submission_id)\n",
    "    except:\n",
    "        submission = reddit.submission(submission_id)\n",
    "    title = submission.title\n",
    "    submission.comments.replace_more() ## loads new page if cooments are multipage\n",
    "    text = [i.body for i in submission.comments]\n",
    "    score = [i.score for i in submission.comments]\n",
    "    user = [i.author for i in submission.comments]\n",
    "    date = [datetime.datetime.fromtimestamp(i.created) for i in submission.comments]\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = text\n",
    "    df['datetime'] = date\n",
    "    df['score'] = score\n",
    "    df['subreddit'] = submission.subreddit\n",
    "    df['redditor'] = user\n",
    "    df['type'] = 'comment'\n",
    "    df['title'] = title\n",
    "    df = df.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising & Lemmatising Function\n",
    "This function does two things: tokenises each comment (breaks it down into individual words and symbols, i.e. tokens) and then lemmatises each word - defaults it down to its basic form. This includes turning plural words singular, adjectives to their root word, etc. standerdising our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function builds on the previous one for ease of use.\n",
    "def lemmatise(reddit_thread):\n",
    "    df = submission(reddit_thread)\n",
    "    df['text'] = [str(i) for i in df['text']]\n",
    "    lemmas_1 = []\n",
    "    for i in df['text']:\n",
    "        tokens_ = word_tokenize(i)\n",
    "        lemmas_ = [lemmatizer.lemmatize(i.lower()) for i in tokens_]\n",
    "        lemmas = [i for i in lemmas_ if i not in stops]\n",
    "        lemmas_1.append(lemmas)\n",
    "    df['words'] = lemmas_1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD Analysis Function\n",
    "The VAD model assigns each word three scores from 0 to 1 - its Valence (how positive it is), Arousal (how stimulating/passionate), and Dominance (how in control it makes one feel). While it is not perfect (new slang words are created all the time, and would likely not be immediately added to the overall model) it can help compare the emotions exhibited in one comment to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = pd.read_excel('vad.xlsx', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function builds on the previous two for ease of use.\n",
    "def VAD_Analysis(reddit_thread):\n",
    "    df = lemmatise(reddit_thread)\n",
    "    vad_ = []\n",
    "    c = [np.nan for i in range(len(vad.columns))]\n",
    "    for i in df['words']:\n",
    "        words = [j for j in i if j in vad.index]\n",
    "        vad_1 = []\n",
    "        for k in words:\n",
    "            try:\n",
    "                vad_1.append(vad.loc[k])\n",
    "            except:\n",
    "                 vad_1.append(c)\n",
    "        vad_df = pd.DataFrame(vad_1, columns = [i for i in vad.columns])\n",
    "        vad_.append(vad_df.mean())\n",
    "    vad_ = pd.DataFrame(vad_)\n",
    "    data = pd.concat([df, vad_], axis =1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Full Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Do not run the cells in this section! This will take a very long time to re-scrape the comments. The next section includes the import of the saved csv file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying our three functions to the top 10 posts of the past year on the subreddit r/worldnews \n",
    "#as gathered on Thursday, 15th February at 14:51:00.\n",
    "post_1 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/123iv0t/norway_sweden_finland_and_denmark_struck_a_deal/')\n",
    "post_2 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/1172vx1/president_biden_makes_surprise_visit_to_ukraine/')\n",
    "post_3 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/129gpui/analysis_of_twitter_algorithm_code_reveals_social/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_4 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/1192qby/biden_vows_to_defend_literally_every_inch_of_nato/')\n",
    "post_5 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/13qz1sw/under_elon_musk_twitter_has_approved_83_of/')\n",
    "post_6 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/118l82r/japan_promises_to_lead_the_world_in_fighting/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_7 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/12bdmyb/nato_gathers_to_welcome_finland_as_31st_member/')\n",
    "post_8 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/1183r5w/putin_falsely_claims_it_was_west_that_started_the/')\n",
    "post_9 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/12csm7u/cisco_systems_pulled_out_of_russia_and_destroyed/')\n",
    "post_10 = VAD_Analysis('https://www.reddit.com/r/worldnews/comments/11aqt3d/lithuanias_prime_minister_says_ukrainians_should/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>redditor</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>words</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This invasion has done more to unite the Europ...</td>\n",
       "      <td>2023-03-27 11:38:10</td>\n",
       "      <td>21695</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>greek_stallion</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>[invasion, ha, done, unite, european, continen...</td>\n",
       "      <td>0.556075</td>\n",
       "      <td>0.565259</td>\n",
       "      <td>0.521931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now you brought the Vikings back together, dam...</td>\n",
       "      <td>2023-03-27 12:37:39</td>\n",
       "      <td>20345</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>Shotguns_x_559</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>[brought, viking, back, together, ,, damn]</td>\n",
       "      <td>0.484521</td>\n",
       "      <td>0.559284</td>\n",
       "      <td>0.495146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2023-03-27 11:58:54</td>\n",
       "      <td>7522</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>None</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>[[, deleted, ]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Introducing NATO+</td>\n",
       "      <td>2023-03-27 13:06:05</td>\n",
       "      <td>6666</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>SpaceToaster</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>[introducing, nato+]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unified Nordic Defense Force sounds like somet...</td>\n",
       "      <td>2023-03-27 13:02:47</td>\n",
       "      <td>4437</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>BMCarbaugh</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>[unified, nordic, defense, force, sound, like,...</td>\n",
       "      <td>0.615946</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.609628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            datetime  \\\n",
       "0  This invasion has done more to unite the Europ... 2023-03-27 11:38:10   \n",
       "1  Now you brought the Vikings back together, dam... 2023-03-27 12:37:39   \n",
       "2                                          [deleted] 2023-03-27 11:58:54   \n",
       "3                                  Introducing NATO+ 2023-03-27 13:06:05   \n",
       "4  Unified Nordic Defense Force sounds like somet... 2023-03-27 13:02:47   \n",
       "\n",
       "   score  subreddit        redditor     type  \\\n",
       "0  21695  worldnews  greek_stallion  comment   \n",
       "1  20345  worldnews  Shotguns_x_559  comment   \n",
       "2   7522  worldnews            None  comment   \n",
       "3   6666  worldnews    SpaceToaster  comment   \n",
       "4   4437  worldnews      BMCarbaugh  comment   \n",
       "\n",
       "                                               title  \\\n",
       "0  Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "1  Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "2  Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "3  Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "4  Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "\n",
       "                                               words   valence   arousal  \\\n",
       "0  [invasion, ha, done, unite, european, continen...  0.556075  0.565259   \n",
       "1         [brought, viking, back, together, ,, damn]  0.484521  0.559284   \n",
       "2                                    [[, deleted, ]]       NaN       NaN   \n",
       "3                               [introducing, nato+]       NaN       NaN   \n",
       "4  [unified, nordic, defense, force, sound, like,...  0.615946  0.598306   \n",
       "\n",
       "   dominance  \n",
       "0   0.521931  \n",
       "1   0.495146  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4   0.609628  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.concat([post_1, post_2, post_3, post_4, post_5, post_6, post_7, post_8, post_9, post_10], axis = 0, ignore_index = True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then clean the corpus from comments that have NaN values\n",
    "#this is better than removing all 'deleted' comments as those still sometimes have text\n",
    "corpus = corpus.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this cell either! This code is showing how I originally stored the corpus\n",
    "corpus.to_csv('csvcorpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>redditor</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>words</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This invasion has done more to unite the Europ...</td>\n",
       "      <td>2023-03-27 11:38:10</td>\n",
       "      <td>21695</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>greek_stallion</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>['invasion', 'ha', 'done', 'unite', 'european'...</td>\n",
       "      <td>0.556075</td>\n",
       "      <td>0.565259</td>\n",
       "      <td>0.521931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now you brought the Vikings back together, dam...</td>\n",
       "      <td>2023-03-27 12:37:39</td>\n",
       "      <td>20345</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>Shotguns_x_559</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>['brought', 'viking', 'back', 'together', ',',...</td>\n",
       "      <td>0.484521</td>\n",
       "      <td>0.559284</td>\n",
       "      <td>0.495146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unified Nordic Defense Force sounds like somet...</td>\n",
       "      <td>2023-03-27 13:02:47</td>\n",
       "      <td>4437</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>BMCarbaugh</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>['unified', 'nordic', 'defense', 'force', 'sou...</td>\n",
       "      <td>0.615946</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.609628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The vikings have returned! But to the skies in...</td>\n",
       "      <td>2023-03-27 11:57:11</td>\n",
       "      <td>3714</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>Ok_Imagination_7119</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>['viking', 'returned', '!', 'sky', 'instead', ...</td>\n",
       "      <td>0.645152</td>\n",
       "      <td>0.411664</td>\n",
       "      <td>0.532160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This would make it one the largest air-forces ...</td>\n",
       "      <td>2023-03-27 11:42:18</td>\n",
       "      <td>2884</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>008Zulu</td>\n",
       "      <td>comment</td>\n",
       "      <td>Norway, Sweden, Finland, and Denmark struck a ...</td>\n",
       "      <td>['would', 'make', 'one', 'largest', 'air-force...</td>\n",
       "      <td>0.681533</td>\n",
       "      <td>0.489880</td>\n",
       "      <td>0.654976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485</th>\n",
       "      <td>We should keep some as a deterrence or for use...</td>\n",
       "      <td>2023-02-24 14:03:02</td>\n",
       "      <td>-21</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>thrownkitchensink</td>\n",
       "      <td>comment</td>\n",
       "      <td>Lithuania's prime minister says Ukrainians sho...</td>\n",
       "      <td>['keep', 'deterrence', 'use', 'direct', 'confl...</td>\n",
       "      <td>0.539720</td>\n",
       "      <td>0.573060</td>\n",
       "      <td>0.589806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>Finally saying the quiet part out loud. Europe...</td>\n",
       "      <td>2023-02-24 16:38:29</td>\n",
       "      <td>-26</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>RickyTicky5309</td>\n",
       "      <td>comment</td>\n",
       "      <td>Lithuania's prime minister says Ukrainians sho...</td>\n",
       "      <td>['finally', 'saying', 'quiet', 'part', 'loud',...</td>\n",
       "      <td>0.586773</td>\n",
       "      <td>0.459753</td>\n",
       "      <td>0.539374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>And the Ukrainian “make a wish” foundation con...</td>\n",
       "      <td>2023-02-24 15:06:17</td>\n",
       "      <td>-32</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>dickchingy</td>\n",
       "      <td>comment</td>\n",
       "      <td>Lithuania's prime minister says Ukrainians sho...</td>\n",
       "      <td>['ukrainian', '“', 'make', 'wish', '”', 'found...</td>\n",
       "      <td>0.700545</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.676847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>get fucked warmonger</td>\n",
       "      <td>2023-02-24 16:30:54</td>\n",
       "      <td>-37</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>tablefourtoo</td>\n",
       "      <td>comment</td>\n",
       "      <td>Lithuania's prime minister says Ukrainians sho...</td>\n",
       "      <td>['get', 'fucked', 'warmonger']</td>\n",
       "      <td>0.626168</td>\n",
       "      <td>0.506861</td>\n",
       "      <td>0.805825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>Except Ukraine is well known for having a blac...</td>\n",
       "      <td>2023-02-24 15:10:47</td>\n",
       "      <td>-42</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>AphexTwins903</td>\n",
       "      <td>comment</td>\n",
       "      <td>Lithuania's prime minister says Ukrainians sho...</td>\n",
       "      <td>['except', 'ukraine', 'well', 'known', 'black'...</td>\n",
       "      <td>0.575701</td>\n",
       "      <td>0.444597</td>\n",
       "      <td>0.620024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4490 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text             datetime  \\\n",
       "0     This invasion has done more to unite the Europ...  2023-03-27 11:38:10   \n",
       "1     Now you brought the Vikings back together, dam...  2023-03-27 12:37:39   \n",
       "2     Unified Nordic Defense Force sounds like somet...  2023-03-27 13:02:47   \n",
       "3     The vikings have returned! But to the skies in...  2023-03-27 11:57:11   \n",
       "4     This would make it one the largest air-forces ...  2023-03-27 11:42:18   \n",
       "...                                                 ...                  ...   \n",
       "4485  We should keep some as a deterrence or for use...  2023-02-24 14:03:02   \n",
       "4486  Finally saying the quiet part out loud. Europe...  2023-02-24 16:38:29   \n",
       "4487  And the Ukrainian “make a wish” foundation con...  2023-02-24 15:06:17   \n",
       "4488                               get fucked warmonger  2023-02-24 16:30:54   \n",
       "4489  Except Ukraine is well known for having a blac...  2023-02-24 15:10:47   \n",
       "\n",
       "      score  subreddit             redditor     type  \\\n",
       "0     21695  worldnews       greek_stallion  comment   \n",
       "1     20345  worldnews       Shotguns_x_559  comment   \n",
       "2      4437  worldnews           BMCarbaugh  comment   \n",
       "3      3714  worldnews  Ok_Imagination_7119  comment   \n",
       "4      2884  worldnews              008Zulu  comment   \n",
       "...     ...        ...                  ...      ...   \n",
       "4485    -21  worldnews    thrownkitchensink  comment   \n",
       "4486    -26  worldnews       RickyTicky5309  comment   \n",
       "4487    -32  worldnews           dickchingy  comment   \n",
       "4488    -37  worldnews         tablefourtoo  comment   \n",
       "4489    -42  worldnews        AphexTwins903  comment   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "1     Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "2     Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "3     Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "4     Norway, Sweden, Finland, and Denmark struck a ...   \n",
       "...                                                 ...   \n",
       "4485  Lithuania's prime minister says Ukrainians sho...   \n",
       "4486  Lithuania's prime minister says Ukrainians sho...   \n",
       "4487  Lithuania's prime minister says Ukrainians sho...   \n",
       "4488  Lithuania's prime minister says Ukrainians sho...   \n",
       "4489  Lithuania's prime minister says Ukrainians sho...   \n",
       "\n",
       "                                                  words   valence   arousal  \\\n",
       "0     ['invasion', 'ha', 'done', 'unite', 'european'...  0.556075  0.565259   \n",
       "1     ['brought', 'viking', 'back', 'together', ',',...  0.484521  0.559284   \n",
       "2     ['unified', 'nordic', 'defense', 'force', 'sou...  0.615946  0.598306   \n",
       "3     ['viking', 'returned', '!', 'sky', 'instead', ...  0.645152  0.411664   \n",
       "4     ['would', 'make', 'one', 'largest', 'air-force...  0.681533  0.489880   \n",
       "...                                                 ...       ...       ...   \n",
       "4485  ['keep', 'deterrence', 'use', 'direct', 'confl...  0.539720  0.573060   \n",
       "4486  ['finally', 'saying', 'quiet', 'part', 'loud',...  0.586773  0.459753   \n",
       "4487  ['ukrainian', '“', 'make', 'wish', '”', 'found...  0.700545  0.490566   \n",
       "4488                     ['get', 'fucked', 'warmonger']  0.626168  0.506861   \n",
       "4489  ['except', 'ukraine', 'well', 'known', 'black'...  0.575701  0.444597   \n",
       "\n",
       "      dominance  \n",
       "0      0.521931  \n",
       "1      0.495146  \n",
       "2      0.609628  \n",
       "3      0.532160  \n",
       "4      0.654976  \n",
       "...         ...  \n",
       "4485   0.589806  \n",
       "4486   0.539374  \n",
       "4487   0.676847  \n",
       "4488   0.805825  \n",
       "4489   0.620024  \n",
       "\n",
       "[4490 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this cell to get the corpus!\n",
    "corpus = pd.read_csv('csvcorpus.csv', index_col = 0)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Possible Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These two cells locate and display the comment most highly rated for Arousal in the corpus.\n",
    "corpus['arousal'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                                              Fuck Russia!\n",
       "datetime                                   2023-03-27 17:24:52\n",
       "score                                                        5\n",
       "subreddit                                            worldnews\n",
       "redditor                                             jay105000\n",
       "type                                                   comment\n",
       "title        Norway, Sweden, Finland, and Denmark struck a ...\n",
       "words                                  ['fuck', 'russia', '!']\n",
       "valence                                               0.569819\n",
       "arousal                                               0.993139\n",
       "dominance                                             0.650485\n",
       "Name: 79, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.iloc[79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that this comment contains a swear, and could indicate avenues for further research: how many swears does each post contain on average? What is the comment with the highest arousal value without a swear? etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word count: 492. No AI tools have been used in the preparation of this submission.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
